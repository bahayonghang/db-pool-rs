# è¿ç»´å’Œç›‘æ§æŒ‡å—

æœ¬æŒ‡å—æä¾› db-pool-rs ç”Ÿäº§ç¯å¢ƒçš„è¿ç»´å’Œç›‘æ§æœ€ä½³å®è·µã€‚

## ğŸ“Š ç›‘æ§ä½“ç³»

### æ ¸å¿ƒæŒ‡æ ‡ç›‘æ§

#### 1. è¿æ¥æ± æŒ‡æ ‡
```python
# è·å–è¿æ¥æ± è¯¦ç»†æŒ‡æ ‡
from db_pool_rs import DatabasePool

pool = DatabasePool()
metrics = await pool.get_detailed_metrics("pool_id")

# å…³é”®æŒ‡æ ‡
print(f"è¿æ¥æ± åˆ©ç”¨ç‡: {metrics.connection_utilization:.2%}")
print(f"æ´»è·ƒè¿æ¥æ•°: {metrics.active_connections}")
print(f"ç­‰å¾…é˜Ÿåˆ—é•¿åº¦: {metrics.waiting_queue_length}")
print(f"è¿æ¥è·å–å»¶è¿Ÿ P99: {metrics.acquire_latency_p99}ms")
```

#### 2. æŸ¥è¯¢æ€§èƒ½æŒ‡æ ‡
```python
# æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡
query_metrics = await pool.get_query_metrics("pool_id")

print(f"QPS: {query_metrics.queries_per_second}")
print(f"æŸ¥è¯¢å»¶è¿Ÿ P50: {query_metrics.latency_p50}ms")
print(f"æŸ¥è¯¢å»¶è¿Ÿ P99: {query_metrics.latency_p99}ms")
print(f"é”™è¯¯ç‡: {query_metrics.error_rate:.2%}")
print(f"æ…¢æŸ¥è¯¢æ•°: {query_metrics.slow_queries_count}")
```

### ç›‘æ§é›†æˆ

#### Prometheus é›†æˆ
```python
# å¯ç”¨ Prometheus æŒ‡æ ‡å¯¼å‡º
from db_pool_rs.monitoring import PrometheusExporter

exporter = PrometheusExporter(
    port=9090,
    metrics_prefix="db_pool_rs",
    export_interval=30  # ç§’
)

pool = DatabasePool(
    monitoring_exporters=[exporter]
)
```

#### Grafana ä»ªè¡¨æ¿é…ç½®
```json
{
  "dashboard": {
    "title": "DB-Pool-RS ç›‘æ§",
    "panels": [
      {
        "title": "è¿æ¥æ± çŠ¶æ€",
        "type": "stat",
        "targets": [
          {
            "expr": "db_pool_rs_connection_pool_active_connections",
            "legendFormat": "æ´»è·ƒè¿æ¥"
          }
        ]
      },
      {
        "title": "æŸ¥è¯¢æ€§èƒ½",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(db_pool_rs_queries_total[5m])",
            "legendFormat": "QPS"
          }
        ]
      }
    ]
  }
}
```

## ğŸš¨ å‘Šè­¦é…ç½®

### å‘Šè­¦è§„åˆ™

#### 1. è¿æ¥æ± å‘Šè­¦
```yaml
# prometheus_alerts.yml
groups:
  - name: db_pool_rs
    rules:
      - alert: HighConnectionUtilization
        expr: db_pool_rs_connection_utilization > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "è¿æ¥æ± åˆ©ç”¨ç‡è¿‡é«˜"
          description: "è¿æ¥æ±  {{ $labels.pool_id }} åˆ©ç”¨ç‡è¾¾åˆ° {{ $value }}%"

      - alert: HighQueryLatency
        expr: db_pool_rs_query_latency_p99 > 1000
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "æŸ¥è¯¢å»¶è¿Ÿè¿‡é«˜"
          description: "è¿æ¥æ±  {{ $labels.pool_id }} P99å»¶è¿Ÿè¾¾åˆ° {{ $value }}ms"

      - alert: HighErrorRate
        expr: rate(db_pool_rs_query_errors_total[5m]) / rate(db_pool_rs_queries_total[5m]) > 0.05
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "æŸ¥è¯¢é”™è¯¯ç‡è¿‡é«˜"
          description: "è¿æ¥æ±  {{ $labels.pool_id }} é”™è¯¯ç‡è¾¾åˆ° {{ $value }}%"
```

#### 2. ç³»ç»Ÿçº§å‘Šè­¦
```python
# è‡ªå®šä¹‰å‘Šè­¦å¤„ç†
from db_pool_rs.monitoring import AlertHandler

class CustomAlertHandler(AlertHandler):
    async def handle_high_latency(self, pool_id: str, latency: float):
        """å¤„ç†é«˜å»¶è¿Ÿå‘Šè­¦"""
        if latency > 2000:  # 2ç§’
            # å‘é€ç´§æ€¥é€šçŸ¥
            await self.send_alert(
                level="critical",
                message=f"è¿æ¥æ±  {pool_id} å»¶è¿Ÿè¾¾åˆ° {latency}ms",
                channels=["slack", "email"]
            )
            # è‡ªåŠ¨æ‰©å®¹
            await self.auto_scale_pool(pool_id)

    async def handle_connection_exhaustion(self, pool_id: str):
        """å¤„ç†è¿æ¥è€—å°½"""
        # ä¸´æ—¶å¢åŠ è¿æ¥æ•°
        await self.emergency_scale_up(pool_id)
        # å‘é€å‘Šè­¦
        await self.send_alert(
            level="critical", 
            message=f"è¿æ¥æ±  {pool_id} è¿æ¥è€—å°½",
            channels=["pagerduty"]
        )

# æ³¨å†Œå‘Šè­¦å¤„ç†å™¨
pool = DatabasePool(
    alert_handlers=[CustomAlertHandler()]
)
```

## ğŸ”§ è¿ç»´æ“ä½œ

### å¥åº·æ£€æŸ¥

#### 1. åŸºç¡€å¥åº·æ£€æŸ¥
```python
async def health_check():
    """åŸºç¡€å¥åº·æ£€æŸ¥"""
    pool = DatabasePool()
    
    # æ£€æŸ¥æ‰€æœ‰è¿æ¥æ± 
    pools = await pool.list_pools()
    healthy_pools = []
    unhealthy_pools = []
    
    for pool_id in pools:
        try:
            # æ‰§è¡Œç®€å•æŸ¥è¯¢éªŒè¯è¿é€šæ€§
            await pool.query(pool_id, "SELECT 1")
            healthy_pools.append(pool_id)
        except Exception as e:
            unhealthy_pools.append((pool_id, str(e)))
    
    return {
        "status": "healthy" if not unhealthy_pools else "degraded",
        "healthy_pools": healthy_pools,
        "unhealthy_pools": unhealthy_pools,
        "timestamp": datetime.utcnow().isoformat()
    }
```

#### 2. æ·±åº¦å¥åº·æ£€æŸ¥
```python
async def deep_health_check(pool_id: str):
    """æ·±åº¦å¥åº·æ£€æŸ¥"""
    pool = DatabasePool()
    
    checks = {
        "connectivity": False,
        "performance": False,
        "resource_usage": False,
        "error_rate": False
    }
    
    # è¿é€šæ€§æ£€æŸ¥
    try:
        await pool.query(pool_id, "SELECT 1")
        checks["connectivity"] = True
    except Exception:
        pass
    
    # æ€§èƒ½æ£€æŸ¥
    start_time = time.time()
    try:
        await pool.query(pool_id, "SELECT 1")
        latency = (time.time() - start_time) * 1000
        checks["performance"] = latency < 100  # 100ms é˜ˆå€¼
    except Exception:
        pass
    
    # èµ„æºä½¿ç”¨æ£€æŸ¥
    metrics = await pool.get_metrics(pool_id)
    checks["resource_usage"] = metrics.connection_utilization < 0.9
    
    # é”™è¯¯ç‡æ£€æŸ¥
    checks["error_rate"] = metrics.error_rate < 0.01
    
    return {
        "pool_id": pool_id,
        "overall_healthy": all(checks.values()),
        "checks": checks,
        "metrics": metrics.to_dict()
    }
```

### æ•…éšœæ¢å¤

#### 1. è‡ªåŠ¨æ•…éšœæ¢å¤
```python
class AutoRecoveryHandler:
    """è‡ªåŠ¨æ•…éšœæ¢å¤å¤„ç†å™¨"""
    
    async def handle_connection_failure(self, pool_id: str, error: Exception):
        """å¤„ç†è¿æ¥æ•…éšœ"""
        print(f"æ£€æµ‹åˆ°è¿æ¥æ•…éšœ: {pool_id} - {error}")
        
        # æ­¥éª¤1: é‡è¯•è¿æ¥
        for attempt in range(3):
            try:
                await self.recreate_pool(pool_id)
                print(f"è¿æ¥æ±  {pool_id} é‡å»ºæˆåŠŸ")
                return
            except Exception as e:
                print(f"é‡è¯• {attempt + 1} å¤±è´¥: {e}")
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
        # æ­¥éª¤2: åˆ‡æ¢åˆ°å¤‡ç”¨è¿æ¥
        backup_pool = await self.get_backup_pool(pool_id)
        if backup_pool:
            await self.switch_to_backup(pool_id, backup_pool)
            print(f"å·²åˆ‡æ¢åˆ°å¤‡ç”¨è¿æ¥æ± : {backup_pool}")
        
        # æ­¥éª¤3: å‘é€å‘Šè­¦
        await self.send_failure_alert(pool_id, error)
    
    async def handle_performance_degradation(self, pool_id: str, metrics):
        """å¤„ç†æ€§èƒ½ä¸‹é™"""
        if metrics.latency_p99 > 1000:  # 1ç§’
            # å¢åŠ è¿æ¥æ•°
            await self.scale_up_connections(pool_id)
        
        if metrics.connection_utilization > 0.9:
            # æ¸…ç†ç©ºé—²è¿æ¥
            await self.cleanup_idle_connections(pool_id)
            # å¢åŠ æœ€å¤§è¿æ¥æ•°
            await self.increase_max_connections(pool_id)
```

#### 2. æ‰‹åŠ¨æ•…éšœæ¢å¤æ“ä½œ
```bash
#!/bin/bash
# scripts/recovery_operations.sh

# é‡å¯ç‰¹å®šè¿æ¥æ± 
restart_pool() {
    local pool_id=$1
    echo "é‡å¯è¿æ¥æ± : $pool_id"
    python -c "
import asyncio
from db_pool_rs import DatabasePool

async def restart():
    pool = DatabasePool()
    await pool.remove_pool('$pool_id')
    # ä»é…ç½®é‡æ–°åˆ›å»º
    await pool.create_pool_from_config('$pool_id')
    print('è¿æ¥æ± é‡å¯å®Œæˆ')

asyncio.run(restart())
"
}

# æ¸…ç†è¿æ¥æ± çŠ¶æ€
cleanup_pool() {
    local pool_id=$1
    echo "æ¸…ç†è¿æ¥æ± çŠ¶æ€: $pool_id"
    python -c "
import asyncio
from db_pool_rs import DatabasePool

async def cleanup():
    pool = DatabasePool()
    await pool.cleanup_pool('$pool_id')
    print('è¿æ¥æ± æ¸…ç†å®Œæˆ')

asyncio.run(cleanup())
"
}

# å¯¼å‡ºè¿æ¥æ± æŒ‡æ ‡
export_metrics() {
    local pool_id=$1
    local output_file=${2:-"metrics_$(date +%Y%m%d_%H%M%S).json"}
    
    echo "å¯¼å‡ºè¿æ¥æ± æŒ‡æ ‡: $pool_id -> $output_file"
    python -c "
import asyncio
import json
from db_pool_rs import DatabasePool

async def export():
    pool = DatabasePool()
    metrics = await pool.get_detailed_metrics('$pool_id')
    with open('$output_file', 'w') as f:
        json.dump(metrics.to_dict(), f, indent=2)
    print('æŒ‡æ ‡å¯¼å‡ºå®Œæˆ')

asyncio.run(export())
"
}
```

## ğŸ“ˆ æ€§èƒ½è°ƒä¼˜

### è¿æ¥æ± è°ƒä¼˜

#### 1. è¿æ¥æ•°ä¼˜åŒ–
```python
# åŠ¨æ€è°ƒæ•´è¿æ¥æ± å¤§å°
async def optimize_pool_size(pool_id: str):
    """æ ¹æ®è´Ÿè½½åŠ¨æ€è°ƒæ•´è¿æ¥æ± å¤§å°"""
    pool = DatabasePool()
    metrics = await pool.get_metrics(pool_id)
    
    # è·å–æœ€è¿‘çš„è´Ÿè½½æ•°æ®
    recent_qps = metrics.queries_per_second
    recent_utilization = metrics.connection_utilization
    
    current_config = await pool.get_pool_config(pool_id)
    
    # è°ƒä¼˜è§„åˆ™
    if recent_utilization > 0.8 and recent_qps > 1000:
        # é«˜è´Ÿè½½ï¼Œå¢åŠ è¿æ¥æ•°
        new_max = min(current_config.max_connections * 1.5, 200)
        await pool.update_pool_config(pool_id, {
            "max_connections": new_max
        })
        
    elif recent_utilization < 0.3 and recent_qps < 100:
        # ä½è´Ÿè½½ï¼Œå‡å°‘è¿æ¥æ•°
        new_max = max(current_config.max_connections * 0.8, 10)
        await pool.update_pool_config(pool_id, {
            "max_connections": new_max
        })
```

#### 2. è¶…æ—¶é…ç½®ä¼˜åŒ–
```python
# æ™ºèƒ½è¶…æ—¶é…ç½®
async def optimize_timeouts(pool_id: str):
    """æ ¹æ®ç½‘ç»œå»¶è¿Ÿè°ƒæ•´è¶…æ—¶é…ç½®"""
    pool = DatabasePool()
    
    # æµ‹é‡ç½‘ç»œå»¶è¿Ÿ
    latencies = []
    for _ in range(10):
        start = time.time()
        try:
            await pool.query(pool_id, "SELECT 1")
            latencies.append((time.time() - start) * 1000)
        except Exception:
            continue
    
    if latencies:
        avg_latency = sum(latencies) / len(latencies)
        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
        
        # è®¾ç½®è¶…æ—¶ä¸º P95 å»¶è¿Ÿçš„ 3 å€
        recommended_timeout = max(p95_latency * 3, 1000)  # æœ€å°‘1ç§’
        
        await pool.update_pool_config(pool_id, {
            "query_timeout": recommended_timeout,
            "acquire_timeout": recommended_timeout * 0.5
        })
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜è¯Šæ–­

#### 1. è¿æ¥æ³„æ¼è¯Šæ–­
```python
async def diagnose_connection_leak(pool_id: str):
    """è¯Šæ–­è¿æ¥æ³„æ¼é—®é¢˜"""
    pool = DatabasePool()
    
    # è·å–è¯¦ç»†è¿æ¥ä¿¡æ¯
    connection_info = await pool.get_connection_details(pool_id)
    
    suspicious_connections = []
    for conn in connection_info:
        # æ£€æŸ¥é•¿æ—¶é—´æ´»è·ƒçš„è¿æ¥
        if conn.active_duration > 300:  # 5åˆ†é’Ÿ
            suspicious_connections.append({
                "connection_id": conn.id,
                "active_duration": conn.active_duration,
                "last_query": conn.last_query,
                "stack_trace": conn.stack_trace
            })
    
    return {
        "total_connections": len(connection_info),
        "suspicious_connections": suspicious_connections,
        "recommendations": generate_leak_recommendations(suspicious_connections)
    }

def generate_leak_recommendations(suspicious_connections):
    """ç”Ÿæˆè¿æ¥æ³„æ¼ä¿®å¤å»ºè®®"""
    recommendations = []
    
    if suspicious_connections:
        recommendations.extend([
            "æ£€æŸ¥åº”ç”¨ä»£ç æ˜¯å¦æ­£ç¡®å…³é—­æ•°æ®åº“è¿æ¥",
            "éªŒè¯å¼‚å¸¸å¤„ç†é€»è¾‘æ˜¯å¦ä¼šå¯¼è‡´è¿æ¥æœªé‡Šæ”¾",
            "è€ƒè™‘å‡å°‘è¿æ¥æœ€å¤§ç”Ÿå­˜æ—¶é—´",
            "å¯ç”¨è¿æ¥æ³„æ¼æ£€æµ‹å’Œè‡ªåŠ¨å›æ”¶"
        ])
    
    return recommendations
```

#### 2. æ€§èƒ½é—®é¢˜è¯Šæ–­
```python
async def diagnose_performance_issues(pool_id: str):
    """è¯Šæ–­æ€§èƒ½é—®é¢˜"""
    pool = DatabasePool()
    
    metrics = await pool.get_detailed_metrics(pool_id)
    slow_queries = await pool.get_slow_queries(pool_id, limit=10)
    
    issues = []
    
    # æ£€æŸ¥é«˜å»¶è¿Ÿ
    if metrics.latency_p99 > 1000:
        issues.append({
            "type": "high_latency",
            "severity": "high",
            "description": f"P99å»¶è¿Ÿè¾¾åˆ° {metrics.latency_p99}ms",
            "recommendations": [
                "æ£€æŸ¥æ•°æ®åº“æœåŠ¡å™¨æ€§èƒ½",
                "ä¼˜åŒ–æ…¢æŸ¥è¯¢",
                "å¢åŠ è¿æ¥æ± å¤§å°",
                "è€ƒè™‘è¯»å†™åˆ†ç¦»"
            ]
        })
    
    # æ£€æŸ¥é«˜é”™è¯¯ç‡
    if metrics.error_rate > 0.05:
        issues.append({
            "type": "high_error_rate", 
            "severity": "critical",
            "description": f"é”™è¯¯ç‡è¾¾åˆ° {metrics.error_rate:.2%}",
            "recommendations": [
                "æ£€æŸ¥æ•°æ®åº“è¿æ¥é…ç½®",
                "éªŒè¯ç½‘ç»œè¿é€šæ€§",
                "æ£€æŸ¥æ•°æ®åº“æœåŠ¡å™¨çŠ¶æ€",
                "å¢åŠ é”™è¯¯é‡è¯•æœºåˆ¶"
            ]
        })
    
    return {
        "issues": issues,
        "slow_queries": slow_queries,
        "overall_health": "healthy" if not issues else "degraded"
    }
```

## ğŸ“‹ è¿ç»´æ£€æŸ¥æ¸…å•

### æ—¥å¸¸æ£€æŸ¥ (æ¯æ—¥)
- [ ] æ£€æŸ¥æ‰€æœ‰è¿æ¥æ± å¥åº·çŠ¶æ€
- [ ] æŸ¥çœ‹é”™è¯¯æ—¥å¿—å’Œå‘Šè­¦
- [ ] éªŒè¯å…³é”®æ€§èƒ½æŒ‡æ ‡
- [ ] ç¡®è®¤å¤‡ä»½å’Œæ¢å¤æœºåˆ¶æ­£å¸¸

### å‘¨æœŸæ£€æŸ¥ (æ¯å‘¨)
- [ ] åˆ†ææ€§èƒ½è¶‹åŠ¿
- [ ] æ£€æŸ¥è¿æ¥æ± é…ç½®æ˜¯å¦éœ€è¦è°ƒæ•´
- [ ] æ¸…ç†è¿‡æœŸçš„ç›‘æ§æ•°æ®
- [ ] éªŒè¯æ•…éšœæ¢å¤æµç¨‹

### æœˆåº¦æ£€æŸ¥ (æ¯æœˆ)
- [ ] å…¨é¢æ€§èƒ½è¯„ä¼°å’Œè°ƒä¼˜
- [ ] æ£€æŸ¥å’Œæ›´æ–°å‘Šè­¦è§„åˆ™
- [ ] å®¹é‡è§„åˆ’å’Œæ‰©å±•è¯„ä¼°
- [ ] å®‰å…¨æ€§æ£€æŸ¥å’Œæ›´æ–°

è¿™ä¸ªè¿ç»´æŒ‡å—æä¾›äº†å®Œæ•´çš„ç›‘æ§ã€å‘Šè­¦ã€æ•…éšœæ¢å¤å’Œæ€§èƒ½è°ƒä¼˜ç­–ç•¥ï¼Œç¡®ä¿ db-pool-rs åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç¨³å®šå¯é åœ°è¿è¡Œã€‚