"""
æ€§èƒ½åŸºå‡†æµ‹è¯• - æ¯”è¾ƒä¸åŒéƒ¨ç½²æ¨¡å¼çš„æ€§èƒ½
"""

import asyncio
import time
import sys
import os
import statistics
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "python"))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BenchmarkRunner:
    def __init__(self, mode="simple"):
        self.mode = mode
        self.pool = None
        
    async def setup(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        os.environ["DB_POOL_MODE"] = self.mode
        
        from db_pool_rs import DatabasePool
        self.pool = DatabasePool(self.mode)
        
        await self.pool.create_pool(
            pool_id="benchmark_pool",
            db_type="mssql",
            host="localhost",
            port=1433,
            database="benchmark_db",
            username="bench_user",
            password="bench_pass",
            min_connections=10,
            max_connections=50
        )
        
    async def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.pool:
            await self.pool.remove_pool("benchmark_pool")
    
    async def benchmark_simple_queries(self, num_queries=1000):
        """åŸºå‡†æµ‹è¯•ï¼šç®€å•æŸ¥è¯¢"""
        logger.info(f"ğŸ” åŸºå‡†æµ‹è¯•: {num_queries} ä¸ªç®€å•æŸ¥è¯¢ ({self.mode}æ¨¡å¼)")
        
        start_time = time.time()
        latencies = []
        
        for i in range(num_queries):
            query_start = time.time()
            try:
                result = await self.pool.query("benchmark_pool", "SELECT 1 as test_value")
                latencies.append((time.time() - query_start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
            except Exception as e:
                logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        return {
            "total_time": total_time,
            "qps": num_queries / total_time,
            "avg_latency_ms": statistics.mean(latencies) if latencies else 0,
            "p99_latency_ms": statistics.quantiles(latencies, n=100)[98] if latencies else 0,
            "min_latency_ms": min(latencies) if latencies else 0,
            "max_latency_ms": max(latencies) if latencies else 0,
        }
    
    async def benchmark_concurrent_queries(self, num_concurrent=10, queries_per_thread=100):
        """åŸºå‡†æµ‹è¯•ï¼šå¹¶å‘æŸ¥è¯¢"""
        logger.info(f"ğŸ”€ åŸºå‡†æµ‹è¯•: {num_concurrent} å¹¶å‘ x {queries_per_thread} æŸ¥è¯¢ ({self.mode}æ¨¡å¼)")
        
        async def worker(worker_id):
            latencies = []
            for i in range(queries_per_thread):
                query_start = time.time()
                try:
                    result = await self.pool.query("benchmark_pool", f"SELECT {worker_id} as worker_id, {i} as query_id")
                    latencies.append((time.time() - query_start) * 1000)
                except Exception as e:
                    logger.error(f"Worker {worker_id} æŸ¥è¯¢ {i} å¤±è´¥: {e}")
            return latencies
        
        start_time = time.time()
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [worker(i) for i in range(num_concurrent)]
        results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # æ±‡æ€»æ‰€æœ‰å»¶è¿Ÿ
        all_latencies = []
        for worker_latencies in results:
            all_latencies.extend(worker_latencies)
        
        total_queries = num_concurrent * queries_per_thread
        
        return {
            "total_time": total_time,
            "qps": total_queries / total_time,
            "avg_latency_ms": statistics.mean(all_latencies) if all_latencies else 0,
            "p99_latency_ms": statistics.quantiles(all_latencies, n=100)[98] if all_latencies else 0,
            "min_latency_ms": min(all_latencies) if all_latencies else 0,
            "max_latency_ms": max(all_latencies) if all_latencies else 0,
            "concurrent_workers": num_concurrent,
        }
    
    async def benchmark_batch_operations(self, num_batches=100, batch_size=10):
        """åŸºå‡†æµ‹è¯•ï¼šæ‰¹é‡æ“ä½œ"""
        logger.info(f"ğŸ“¦ åŸºå‡†æµ‹è¯•: {num_batches} æ‰¹æ¬¡ x {batch_size} æ“ä½œ ({self.mode}æ¨¡å¼)")
        
        start_time = time.time()
        
        for batch_id in range(num_batches):
            operations = []
            for op_id in range(batch_size):
                operations.append(f"INSERT INTO benchmark_log VALUES ({batch_id}, {op_id}, 'test')")
            
            try:
                results = await self.pool.execute_batch("benchmark_pool", operations)
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {batch_id} å¤±è´¥: {e}")
        
        end_time = time.time()
        total_time = end_time - start_time
        total_operations = num_batches * batch_size
        
        return {
            "total_time": total_time,
            "operations_per_second": total_operations / total_time,
            "batches_per_second": num_batches / total_time,
            "avg_batch_time_ms": (total_time / num_batches) * 1000,
        }

async def run_benchmarks():
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    print("ğŸš€ DB-Pool-RS æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    modes = ["simple"]  # å½“å‰åªæœ‰simpleæ¨¡å¼å¯ç”¨
    
    results = {}
    
    for mode in modes:
        print(f"\nğŸ“Š æµ‹è¯• {mode.upper()} æ¨¡å¼")
        print("-" * 40)
        
        runner = BenchmarkRunner(mode)
        
        try:
            await runner.setup()
            
            # ç®€å•æŸ¥è¯¢åŸºå‡†
            simple_result = await runner.benchmark_simple_queries(1000)
            print(f"  ç®€å•æŸ¥è¯¢ (1000æ¬¡):")
            print(f"    QPS: {simple_result['qps']:.2f}")
            print(f"    å¹³å‡å»¶è¿Ÿ: {simple_result['avg_latency_ms']:.2f}ms")
            print(f"    P99å»¶è¿Ÿ: {simple_result['p99_latency_ms']:.2f}ms")
            
            # å¹¶å‘æŸ¥è¯¢åŸºå‡†
            concurrent_result = await runner.benchmark_concurrent_queries(10, 100)
            print(f"  å¹¶å‘æŸ¥è¯¢ (10å¹¶å‘ x 100æ¬¡):")
            print(f"    QPS: {concurrent_result['qps']:.2f}")
            print(f"    å¹³å‡å»¶è¿Ÿ: {concurrent_result['avg_latency_ms']:.2f}ms")
            print(f"    P99å»¶è¿Ÿ: {concurrent_result['p99_latency_ms']:.2f}ms")
            
            # æ‰¹é‡æ“ä½œåŸºå‡†
            batch_result = await runner.benchmark_batch_operations(50, 20)
            print(f"  æ‰¹é‡æ“ä½œ (50æ‰¹æ¬¡ x 20æ“ä½œ):")
            print(f"    æ“ä½œ/ç§’: {batch_result['operations_per_second']:.2f}")
            print(f"    æ‰¹æ¬¡/ç§’: {batch_result['batches_per_second']:.2f}")
            print(f"    å¹³å‡æ‰¹æ¬¡æ—¶é—´: {batch_result['avg_batch_time_ms']:.2f}ms")
            
            results[mode] = {
                "simple_queries": simple_result,
                "concurrent_queries": concurrent_result,
                "batch_operations": batch_result,
            }
            
        except Exception as e:
            logger.error(f"{mode}æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
            results[mode] = {"error": str(e)}
        
        finally:
            await runner.cleanup()
    
    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    print(f"\nğŸ“‹ æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("=" * 60)
    
    print(f"{'æ¨¡å¼':<10} {'ç®€å•æŸ¥è¯¢QPS':<15} {'å¹¶å‘æŸ¥è¯¢QPS':<15} {'æ‰¹é‡æ“ä½œ/ç§’':<15}")
    print("-" * 60)
    
    for mode, result in results.items():
        if "error" not in result:
            simple_qps = result["simple_queries"]["qps"]
            concurrent_qps = result["concurrent_queries"]["qps"]
            batch_ops = result["batch_operations"]["operations_per_second"]
            print(f"{mode.upper():<10} {simple_qps:<15.2f} {concurrent_qps:<15.2f} {batch_ops:<15.2f}")
        else:
            print(f"{mode.upper():<10} {'ERROR':<15} {'ERROR':<15} {'ERROR':<15}")

if __name__ == "__main__":
    asyncio.run(run_benchmarks())